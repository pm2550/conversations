# é«˜çº§æ•°æ®å¤„ç†å’Œä¼˜åŒ–
def create_persona_aware_samples():
    """åˆ›å»ºæ„ŸçŸ¥ä¸ªæ€§çš„è®­ç»ƒæ ·æœ¬"""
    print("åˆ›å»ºä¸ªæ€§åŒ–è®­ç»ƒæ ·æœ¬...")
    
    # åŠ è½½åŸå§‹æ ·æœ¬
    samples = []
    with open('deepseek_data.jsonl', 'r', encoding='utf-8') as f:
        for line in f:
            samples.append(json.loads(line))
    
    # åˆ†æç›®æ ‡ç”¨æˆ·(é›·ğŸ·ğŸ·)çš„è¯­è¨€ç‰¹å¾
    target_responses = [s['completion'] for s in samples]
    
    # ç‰¹å¾åˆ†æ
    features = {
        "å¸¸ç”¨è¯æ±‡": ["dds", "æ„šè ¢", "ğŸ·", "å¤ªæ„šè ¢äº†"],
        "å¸¸ç”¨è¡¨è¾¾": ["å‘¼å“ˆå“ˆ", "ä¸¢å¤§å¸ˆ", "çš®è‚¤å…¨æ˜¯æ„šè ¢"],  
        "è¯­è¨€é£æ ¼": "ç®€çŸ­ã€ç›´æ¥ã€å¸¦æœ‰æ¸¸æˆç”¨è¯­",
        "è¡¨æƒ…ä½¿ç”¨": "å¸¸ç”¨ğŸ·å’Œ[è¡¨æƒ…]",
        "å¹³å‡é•¿åº¦": sum(len(r) for r in target_responses) / len(target_responses)
    }
    
    print("è¯­è¨€ç‰¹å¾åˆ†æ:")
    for key, value in features.items():
        print(f"  {key}: {value}")
    
    # åˆ›å»ºå¢å¼ºæ ·æœ¬
    enhanced_samples = []
    for sample in samples:
        # åŸºç¡€æ ·æœ¬
        enhanced_samples.append(sample)
        
        # å¦‚æœæ˜¯å¤šäººå¯¹è¯ï¼Œåˆ›å»ºé’ˆå¯¹æ€§å˜ä½“
        prompt = sample['prompt']
        if 'å¯¹è¯å†å²:' in prompt:
            # æå–å¯¹è¯å‚ä¸è€…
            history_part = prompt.split('å¯¹è¯å†å²:')[1].split('é›·ğŸ·ğŸ·:')[0]
            speakers = list(set([line.split(':')[0] for line in history_part.split('\n') if ':' in line]))
            
            # ä¸ºæ¯ä¸ªä¸»è¦å¯¹è¯è€…åˆ›å»ºä¸ªæ€§åŒ–æç¤º
            for speaker in speakers[:2]:  # é™åˆ¶å‰2ä¸ªä¸»è¦å¯¹è¯è€…
                if speaker.strip() and speaker != 'é›·ğŸ·ğŸ·':
                    persona_prompt = prompt.replace(
                        'é›·ğŸ·ğŸ·: ', 
                        f'é›·ğŸ·ğŸ· å¯¹ {speaker}: '
                    )
                    enhanced_samples.append({
                        'prompt': persona_prompt,
                        'completion': sample['completion']
                    })
    
    # ä¿å­˜å¢å¼ºæ ·æœ¬
    with open('enhanced_deepseek_data.jsonl', 'w', encoding='utf-8') as f:
        for sample in enhanced_samples:
            f.write(json.dumps(sample, ensure_ascii=False) + '\n')
    
    print(f"åŸå§‹æ ·æœ¬: {len(samples)}")
    print(f"å¢å¼ºæ ·æœ¬: {len(enhanced_samples)}")
    print("å·²ä¿å­˜åˆ° enhanced_deepseek_data.jsonl")
    
    return enhanced_samples

def analyze_conversation_styles():
    """åˆ†æä¸åŒç¾¤ç»„çš„å¯¹è¯é£æ ¼å·®å¼‚"""
    print("åˆ†æå¯¹è¯é£æ ¼...")
    
    with open('chat_patterns.json', 'r', encoding='utf-8') as f:
        patterns = json.load(f)
    
    # æŒ‰äº’åŠ¨é¢‘ç‡æ’åº
    sorted_friends = sorted(patterns.items(), 
                           key=lambda x: x[1]['interaction_count'], 
                           reverse=True)
    
    print("ä¸»è¦èŠå¤©å¯¹è±¡åˆ†æ:")
    print("-" * 60)
    for friend, data in sorted_friends[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
        print(f"æœ‹å‹: {friend}")
        print(f"  äº’åŠ¨æ¬¡æ•°: {data['interaction_count']}")
        print(f"  å¹³å‡å›å¤é•¿åº¦: {data['avg_response_length']:.1f}å­—ç¬¦")
        print(f"  è¡¨æƒ…ä½¿ç”¨é¢‘ç‡: {data['emoji_usage']:.2f}")
        print(f"  å¸¸ç”¨è¯æ±‡: {data['common_words'][:3]}")
        print(f"  æ´»è·ƒç¾¤ç»„: {data['groups']}")
        print("-" * 30)

# è¿è¡Œåˆ†æ
if __name__ == "__main__":
    create_persona_aware_samples()
    analyze_conversation_styles()
    
    print("\nä½¿ç”¨å»ºè®®:")
    print("1. ä½¿ç”¨ enhanced_deepseek_data.jsonl è¿›è¡Œè®­ç»ƒä»¥è·å¾—æ›´å¥½æ•ˆæœ")
    print("2. å¯ä»¥æ ¹æ®chat_patterns.jsonè°ƒæ•´ä¸åŒæœ‹å‹çš„å¯¹è¯ç­–ç•¥")
    print("3. å»ºè®®è®­ç»ƒæ—¶å¢åŠ epochæ•°åˆ°6-8è½®ä»¥å……åˆ†å­¦ä¹ ä¸ªæ€§ç‰¹å¾")
